{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Bibliotecas usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV,cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Exploração dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando a estrura do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('default_of_credit_card_clients__courseware_version_1_21_19.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f'A Quantidade de linhas nesse data frame é {df.shape[0]}. Enquanto a quantidade de colunas é de {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f'Temos um total de {sum(df.dtypes == \"int64\")} variáveis inteiras e {sum(df.dtypes == \"object\")} do tipo objeto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisar as informações acima podemos observar que a quantidade dos dados presentes no dataframe está de acordo com o dicionário dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando se na coluna \"id\" existe valores duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f'A quantidade de valores duplicados é de {(df.shape[0])-(df[\"ID\"].nunique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando os dados que estão duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "id_counts = df['ID'].value_counts()\n",
    "duplicados = id_counts == 2\n",
    "duplicados = id_counts.index[duplicados]\n",
    "print(f'Verificando a quantidade de duplicados {len(duplicados)}.')\n",
    "df.loc[df['ID'].isin(duplicados),:].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os id's que estão duplicados tem, em alguma coluna, valores igual a zero. Sendo que em algumas delas esses zeros não deveria existir. Então não faz sentido manter esses valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluindo as linhas que tem, em todas as colunas, valores igual a zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sem_zeros = df == 0\n",
    "zeros_mask = sem_zeros.iloc[:,1:].all(axis=1)\n",
    "df_clean_1 = df.loc[~zeros_mask,:].copy()\n",
    "print(f'A quantidade de valores unicos era de {df[\"ID\"].nunique()}')\n",
    "print(f'Sem os valores duplicados, a quantidade de dados agora é de {len(df_clean_1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando e modificando os dados categóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável \"SEX\" não vai ser utilizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_1.drop('SEX', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EDUCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_1['EDUCATION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável EDUCATION tem, segundo as informações do dicionário, as variáveis: 1 = pós-graduação; 2 = universidade; 3 = ensino médio; 4 = outros.\n",
    "\n",
    "Portanto, não faz sentindo manter outros valores diferentes desse. Assim, o outros valores serão adicionados no número 4, \"outros\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_1['EDUCATION'].replace(to_replace=[0,5,6], value = 4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_1['EDUCATION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MARRIAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_1['MARRIAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quase que da mesma forma que a variável anterior, tem um valor que não identificado ao analisar o dicionário de dados.\n",
    "\n",
    "Assim sendo, o 1 = Casado; 2 = Solteiro ; 3 = Outros.\n",
    "\n",
    "O valor 4 não é identificado no dicionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_1['MARRIAGE'].replace(to_replace = 4, value=3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_1['MARRIAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mapeando os dados categoricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDUCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "edu_map = {\n",
    "    1:'pos-graduacao',\n",
    "    2:'graduacao',\n",
    "    3:'medio',\n",
    "    4:'outros'\n",
    "}\n",
    "\n",
    "df_clean_1['EDU_CAT'] = df_clean_1['EDUCATION'].map(edu_map)\n",
    "edu_ohe = pd.get_dummies(df_clean_1['EDU_CAT'])\n",
    "df_clean_1 = pd.concat([df_clean_1, edu_ohe], axis=1)\n",
    "df_clean_1[['EDUCATION','EDU_CAT','pos-graduacao','graduacao','medio','outros']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MARRIAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_1.MARRIAGE.value_counts()\n",
    "\n",
    "mar_map = {\n",
    "    1:'casado',\n",
    "    2:'solteiro',\n",
    "    3:'outros'\n",
    "}\n",
    "\n",
    "df_clean_1['MAR_CAT'] = df_clean_1['MARRIAGE'].map(mar_map)\n",
    "df_clean_1['MAR_CAT'].value_counts()\n",
    "mar_ohe = pd.get_dummies(df_clean_1['MAR_CAT'])\n",
    "df_clean_1 = pd.concat([df_clean_1, mar_ohe], axis=1)\n",
    "df_clean_1[['MARRIAGE', 'MAR_CAT','casado','solteiro','outros']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando dados continuos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando os dados de pagamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modificando a coluna \"PAY_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_1['PAY_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pay_clean = df_clean_1['PAY_1'] != 'Not available'\n",
    "df_clean_2 = df_clean_1.loc[pay_clean,:].copy()\n",
    "df_clean_2['PAY_1'] = df_clean_2['PAY_1'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_2[['PAY_1','PAY_2']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_2['PAY_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analise geral dos dados de pagamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. PAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "serie_pagamento = [\"PAY_1\",\"PAY_2\",'PAY_3','PAY_4',\"PAY_5\",\"PAY_6\"]\n",
    "df_clean_2[serie_pagamento].hist(bins=np.array(range(-2,10)), layout=(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente somente a coluna PAY_1 será valida para a construção do modelo. Isso se deve ao fato de, ao analisar os gráficos, perceber que todas as outras variáveis são, aproximadamente, igual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_3 = df_clean_2.drop([\"PAY_2\",'PAY_3','PAY_4',\"PAY_5\",\"PAY_6\"], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. BILL_PAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bills = ['BILL_AMT1',\n",
    "       'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\n",
    "\n",
    "df_clean_3[bills].apply(np.log10).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#df_clean_3[bills].apply(np.log10).hist(bins=20, layout=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_3[bills].apply(np.log10).plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que faria a pessoa ter um saldo de crédito negativo? O banco está devendo para ela?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. PAY AMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pay_amt =['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "df_clean_3[pay_amt].apply(np.log10).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#df_clean_3[pay_amt].apply(np.log10).hist(bins=20,layout=(2,3),ylabelsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_3[pay_amt].apply(np.log10).plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Salvando o data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_clean_2.to_csv('df_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Regressão logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df_clean_2[['PAY_1','LIMIT_BAL']]\n",
    "y = df_clean_2['default payment next month']\n",
    "\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_rg = LogisticRegression(solver='liblinear')\n",
    "log_rg.fit(x_treino, y_treino)\n",
    "y_pred = log_rg.predict(x_teste)\n",
    "y_pred_prob = log_rg.predict_proba(x_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculando as probabilidades dos valores manualmente\n",
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y\n",
    "\n",
    "coef_int = np.concatenate([log_rg.intercept_.reshape(1,1), log_rg.coef_], axis=1)\n",
    "ones = np.hstack([np.ones((x_teste.shape[0],1)), x_teste])\n",
    "x_lin_comb = np.dot(coef_int, np.transpose(ones))\n",
    "y_pred_prob_manual = sigmoid(x_lin_comb)\n",
    "y_pred_manual = y_pred_prob_manual >= 0.5\n",
    "\n",
    "# Verificando se os array que foi calculado manualmente ou através da função são iguais\n",
    "print(np.array_equal(y_pred.reshape(1,-1), y_pred_manual))\n",
    "\n",
    "# Calculandoo as ROC AUC's\n",
    "roc_auc_score(y_true=y_teste,y_score=y_pred_prob[:,1])\n",
    "\n",
    "# Quanto mais proximo de 0.5 pior é o modelo, enquanto que quanto mais perto de 1 melhor. Nesse caso, tem-se muito o que melhorar..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Validação cruzada, regularização e tunagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instanciando o modelo de Reg logistica\n",
    "new_log_rg = LogisticRegression(solver='saga', penalty = 'l1', max_iter = 1000)\n",
    "\n",
    "# instanciando o fold\n",
    "k_folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "\n",
    "# instanciando o MinMaxScaler\n",
    "min_max_sc = MinMaxScaler()\n",
    "\n",
    "# Criando a pipeline\n",
    "scaler_lr_pipeline = Pipeline(steps = [('scaler', min_max_sc), ('model', new_log_rg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_val_C_search_pipe(k_folds, C_vals, pipeline, X, Y):\n",
    "    \n",
    "    n_folds = k_folds.n_splits\n",
    "    cv_train_roc_auc = np.empty((n_folds, len(C_vals)))\n",
    "    cv_test_roc_auc = np.empty((n_folds, len(C_vals)))\n",
    "    cv_test_roc = [[]]*len(C_vals)\n",
    "\n",
    "    for c_val_counter in range(len(C_vals)):\n",
    "        #Set the C value for the model object\n",
    "        pipeline.set_params(model__C = C_vals[c_val_counter])\n",
    "        #Count folds for each value of C\n",
    "        fold_counter = 0\n",
    "        #Get training and testing indices for each fold\n",
    "        for train_index, test_index in k_folds.split(X, Y):\n",
    "            #Subset the features and response, for training and testing data for\n",
    "            #this fold\n",
    "            X_cv_train, X_cv_test = X[train_index], X[test_index]\n",
    "            y_cv_train, y_cv_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            #Fit the model on the training data\n",
    "            pipeline.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "            #Get the training ROC AUC\n",
    "            y_cv_train_predict_proba = pipeline.predict_proba(X_cv_train)\n",
    "            cv_train_roc_auc[fold_counter, c_val_counter] = \\\n",
    "            roc_auc_score(y_cv_train, y_cv_train_predict_proba[:,1])\n",
    "\n",
    "            #Get the testing ROC AUC\n",
    "            y_cv_test_predict_proba = pipeline.predict_proba(X_cv_test)\n",
    "            cv_test_roc_auc[fold_counter, c_val_counter] = \\\n",
    "            roc_auc_score(y_cv_test, y_cv_test_predict_proba[:,1])\n",
    "\n",
    "            #Testing ROC curves for each fold\n",
    "            this_fold_roc = roc_curve(y_cv_test, y_cv_test_predict_proba[:,1])\n",
    "            cv_test_roc[c_val_counter].append(this_fold_roc)\n",
    "\n",
    "            #Increment the fold counter\n",
    "            fold_counter += 1\n",
    "\n",
    "        #Indicate progress\n",
    "        print('Done with C = {}'.format(pipeline.get_params()['model__C']))\n",
    "\n",
    "    return cv_train_roc_auc, cv_test_roc_auc, cv_test_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecionando as variáveis\n",
    "x = df_clean_3[['LIMIT_BAL',\n",
    " 'EDUCATION',\n",
    " 'MARRIAGE',\n",
    " 'AGE',\n",
    " 'PAY_1',\n",
    " 'BILL_AMT1',\n",
    " 'BILL_AMT2',\n",
    " 'BILL_AMT3',\n",
    " 'BILL_AMT4',\n",
    " 'BILL_AMT5',\n",
    " 'BILL_AMT6',\n",
    " 'PAY_AMT1',\n",
    " 'PAY_AMT2',\n",
    " 'PAY_AMT3',\n",
    " 'PAY_AMT4',\n",
    " 'PAY_AMT5',\n",
    " 'PAY_AMT6']]\n",
    "\n",
    "y = df_clean_3['default payment next month'].replace(to_replace=4,value=0)\n",
    "\n",
    "# Separando o dataset\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x.values, y.values, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cv_train_roc_auc, cv_test_roc_auc, cv_test_roc = cross_val_C_search_pipe(k_folds=k_folds, C_vals=c_values, pipeline = scaler_lr_pipeline, X = x_treino, Y=y_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#plt.plot([2,1,0,-1,-2,-3],np.mean(cv_train_roc_auc, axis=0), '-o', label = 'treino')\n",
    "#plt.plot([2,1,0,-1,-2,-3],np.mean(cv_test_roc_auc, axis=0), '-x', label = 'teste')\n",
    "#plt.ylabel('ROC AUC')\n",
    "#plt.xlabel('log$_{10}$(C)')\n",
    "#plt.legend()\n",
    "\n",
    "# Pouca diferença entre os valores, ou seja, o over ou under fit \"não existe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando caracteristicas de intereção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Instacioando o elemento e fazendo as interações\n",
    "interacoes = PolynomialFeatures(degree=2,interaction_only=True, include_bias = False)\n",
    "f_interacoes = interacoes.fit_transform(x.values)\n",
    "\n",
    "# Separando os dados\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(f_interacoes, y.values, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Refazendo o teste com os folds\n",
    "# comentado para que não haja necessidade de rodar toda vez, já que é uma etapa que demora\n",
    "#cv_train_roc_auc, cv_test_roc_auc, cv_test_roc = cross_val_C_search_pipe(k_folds=k_folds, C_vals=c_values, pipeline = scaler_lr_pipeline, X = x_treino, Y=y_treino)\n",
    "\n",
    "\n",
    "# Construindo o gráfico com as novas features\n",
    "#plt.plot([2,1,0,-1,-2,-3],np.mean(cv_train_roc_auc, axis=0), '-o', label = 'treino')\n",
    "#plt.plot([2,1,0,-1,-2,-3],np.mean(cv_test_roc_auc, axis=0), '-x', label = 'teste')\n",
    "#plt.ylabel('ROC AUC')\n",
    "#plt.xlabel('log$_{10}$(C)')\n",
    "#plt.legend()\n",
    "\n",
    "\n",
    "# Pouca diferença com essas novas features..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Árvore de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instaciando o modelo\n",
    "dt = tree.DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# Dados que serão utilizados\n",
    "x = df_clean_3[['LIMIT_BAL', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_1', 'BILL_AMT1',\n",
    "       'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "       'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]\n",
    "\n",
    "y = df_clean_3[['default payment next month']]\n",
    "\n",
    "# Separação dos dados\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x,y,test_size=0.2,random_state=1)\n",
    "\n",
    "# Treinando o modelo\n",
    "dt = dt.fit(x_treino, y_treino)\n",
    "\n",
    "# Visualizando o gráfico criado\n",
    "dt_data = tree.export_graphviz(dt, out_file=None, filled=True, rounded=True, feature_names=x.columns, \n",
    "                               proportion=True,class_names=['Not defaulted', 'Defaulted'])\n",
    "\n",
    "# Renderizando a imagem\n",
    "graphviz.Source(dt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Florestas aleatórias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instaciando o modelo\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Configurando os parametros para ser testados\n",
    "params = {'max_depth':[3,6,9,12], 'n_estimators':[50,100,200]}\n",
    "\n",
    "# Instaciando o gridsearch e treinando\n",
    "grid_cv = GridSearchCV(rf, param_grid=params, scoring='roc_auc', cv=4,verbose=1, pre_dispatch=None,error_score=np.nan, return_train_score=True)\n",
    "grid_cv.fit(x_treino, y_treino)\n",
    "\n",
    "# Melhores paramatros para esses dados\n",
    "grid_cv.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preenchendo os valores faltantes do dataset inicial para retreino"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um modelo para prever os dados faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando os dados\n",
    "y = x['PAY_1']\n",
    "x = x.drop('PAY_1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os dados\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x.values,y,test_size=0.2, random_state=1)\n",
    "\n",
    "# Parametros que serão testados\n",
    "rf_params = {'max_depth':[3,6,9,12], 'n_estimators':[10,50,100,200]}\n",
    "\n",
    "# Instanciando o floresta randomica\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Instanciando e executando o GridSeachCV\n",
    "cv_rf_input = GridSearchCV(rf,param_grid=rf_params, scoring='accuracy', n_jobs=1, refit=True,cv=4,\n",
    "                           verbose=2,error_score=np.nan, return_train_score=True)\n",
    "cv_rf_input.fit(x_treino, y_treino)\n",
    "\n",
    "# Melhores parametros e resultado para esse modelo\n",
    "display(cv_rf_input.best_params_)\n",
    "display(cv_rf_input.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando se a acurácia está correta com o valor dos dados de teste\n",
    "y_impute_predict = cv_rf_input.predict(x_teste)\n",
    "accuracy_score(y_pred=y_impute_predict, y_true=y_teste)\n",
    "\n",
    "# Os valores são compativeis, ou seja, não está overfitado ou underfitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando gráfico dessa previsão para ver se há compatibilidade\n",
    "fig, axs = plt.subplots(1,2, figsize=(8,3))\n",
    "axs[0].hist(y_teste)\n",
    "axs[0].set_title('Pay 1 sem valores faltantes')\n",
    "axs[1].hist(y_impute_predict)\n",
    "axs[1].set_title('Pay 1 do modelo (com dados inputados)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevendo os valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancionando um novo modelo com os parametros ajustados e treinando-o com todos os dados disponveis para prever a variável pay_1\n",
    "rf_imput = RandomForestClassifier(n_estimators=100, max_depth=12)\n",
    "rf_imput.fit(df_clean_3[['LIMIT_BAL', 'EDUCATION', 'MARRIAGE', 'AGE', 'BILL_AMT1',\n",
    "       'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "       'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']], df_clean_3['PAY_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando o dataframe para só ter os valores que precisão ser ajustados\n",
    "na_pay1 = df_clean_1['PAY_1'] == 'Not available'\n",
    "df_pay1_na = df_clean_1.loc[na_pay1,:]\n",
    "df_pay1_na = df_pay1_na[['LIMIT_BAL', 'EDUCATION', 'MARRIAGE', 'AGE', 'BILL_AMT1',\n",
    "       'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "       'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]\n",
    "\n",
    "# Prevendo os valores para PAY_1\n",
    "df_pay1_na['PAY_1'] = rf_imput.predict(df_pay1_na.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando os resultados (impacto no modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando os valores da variavel dependente\n",
    "na_pay1 = df_clean_1['PAY_1'] == 'Not available'\n",
    "defaut_na_pay = df_clean_1.loc[na_pay1,:]\n",
    "defaut = defaut_na_pay['default payment next month']\n",
    "\n",
    "x = df_clean_3[['LIMIT_BAL', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_1', 'BILL_AMT1',\n",
    "       'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "       'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]\n",
    "\n",
    "y = df_clean_3['default payment next month']\n",
    "\n",
    "# Fazendo uma nova separação dos dados sem os valore\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x,y,random_state=1, test_size=0.2)\n",
    "\n",
    "# Fazer uma nova separação de dados com os valores previstos\n",
    "x_fill_pay1_treino,x_fill_pay1_teste,y_fill_pay1_treino, y_fill_pay1_teste = train_test_split(df_pay1_na,defaut, test_size=0.2, random_state=1)\n",
    "\n",
    "# Combinando os dados \n",
    "x_treino_completo = np.concatenate((x_treino, x_fill_pay1_treino), axis=0)\n",
    "y_treino_completo = np.concatenate((y_treino, y_fill_pay1_treino), axis=0)\n",
    "x_teste_completo = np.concatenate((x_teste, x_fill_pay1_teste), axis=0)\n",
    "y_teste_completo = np.concatenate((y_teste, y_fill_pay1_teste), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo a analise de resultado \n",
    "#tem que ter resultado proximo ao que foi feito anteriormente, quando foi encontrado os melhores parametros da Arvore de decisão\n",
    "inputation_compare_cv = cross_validate(rf,x_treino_completo, y_treino_completo, scoring='roc_auc', cv =4 , n_jobs=1,verbose=1, return_train_score=True, \n",
    "                                       return_estimator=True, error_score='raise')\n",
    "np.mean(inputation_compare_cv['test_score'])\n",
    "\n",
    "# o resultado está bem proximo ao que foi treinado anteriormente... portanto, não houve queda ao imputar os novos valores..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7634087763033293"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo com os melhores parametros encontrado \n",
    "rf = RandomForestClassifier(max_depth=12, n_estimators=100)\n",
    "\n",
    "# Treinando com todos as observações do dataset\n",
    "rf.fit(x_treino_completo,y_treino_completo)\n",
    "\n",
    "# Metricas\n",
    "y_teste_pred_prob = rf.predict_proba(x_teste_completo)\n",
    "roc_auc_score(y_teste_completo, y_teste_pred_prob[:,1])\n",
    "\n",
    "# Modelo sem over e under fit. Resultado como esperado. Modelo robusto e pronto para o uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0726c0838a80937985c3340d5b14f3c0cf281a70241aa1f5a58b019cd9f50f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
